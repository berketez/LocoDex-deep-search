import asyncio
import aiohttp
import json
from datetime import datetime
import os
import logging

logger = logging.getLogger(__name__)

class RealDeepResearcher:
    """GerÃ§ek web aramasÄ± yapan deep research sistemi"""
    
    def __init__(self, model_name, model_source, websocket):
        self.model_name = model_name
        self.model_source = model_source
        self.websocket = websocket
        self.search_results = []
        
        # GÃ¼venilir kaynak listeleri
        self.trusted_domains = {
            'high': [
                'arxiv.org', 'nature.com', 'science.org', 'pubmed.ncbi.nlm.nih.gov',
                'ieee.org', 'acm.org', 'academic.oup.com', 'springer.com',
                'cambridge.org', 'mit.edu', 'stanford.edu', 'harvard.edu',
                'openai.com', 'deepmind.com', 'anthropic.com', 'microsoft.com',
                'google.com', 'apple.com', 'nvidia.com', 'meta.com',
                'techcrunch.com', 'arstechnica.com', 'wired.com', 'zdnet.com',
                'reuters.com', 'bbc.com', 'cnn.com', 'nytimes.com',
                'github.com', 'stackoverflow.com', 'medium.com', 'substack.com',
                'huggingface.co', 'kaggle.com', 'paperswithcode.com',
                'who.int', 'cdc.gov', 'nasa.gov', 'fda.gov', 'sec.gov'
            ],
            'medium': [
                'wikipedia.org', 'reddit.com', 'quora.com', 'forbes.com',
                'businessinsider.com', 'theverge.com', 'engadget.com',
                'venturebeat.com', 'techrepublic.com', 'pcmag.com',
                'blog.google.com', 'engineering.fb.com', 'blogs.microsoft.com',
                'aws.amazon.com', 'cloud.google.com', 'azure.microsoft.com'
            ],
            'low': [
                'blog.', 'wordpress.com', 'blogspot.com', 'tumblr.com',
                'yahoo.com', 'answers.com', 'ehow.com'
            ]
        }
        
        # GÃ¼vensiz/spam domain'ler
        self.untrusted_domains = [
            'clickbait.com', 'spam.com', 'fake-news.com', 'ads.com',
            'affiliate.com', 'referral.com', 'promotion.com'
        ]
    
    async def evaluate_source_reliability(self, url, title, content_sample, topic):
        """Model ile kaynak gÃ¼venilirliÄŸi deÄŸerlendirir"""
        try:
            from datetime import datetime
            current_date = datetime.now().strftime("%Y-%m-%d")
            
            reliability_prompt = f"""
Bu web kaynaÄŸÄ±nÄ±n gÃ¼venilirliÄŸini deÄŸerlendir:

URL: {url}
BaÅŸlÄ±k: {title}
Ä°Ã§erik Ã–rneÄŸi: {content_sample[:500]}
AraÅŸtÄ±rma Konusu: {topic}
BugÃ¼nÃ¼n Tarihi: {current_date}

Ã–ZEL DEÄERLENDÄ°RME KRÄ°TERLERÄ°:

1. **Konu TÃ¼rÃ¼ Analizi (Ã‡OK Ã–NEMLÄ°):**
   - Ã–nce konuyu kategorize et:
     * Teknoloji/AI: HÄ±zlÄ± deÄŸiÅŸen, gÃ¼ncellik kritik
     * Tarih/Psikoloji: YavaÅŸ deÄŸiÅŸen, akademik kaynaklar Ã¶ncelikli
     * Siyaset: TarafsÄ±zlÄ±k kritik, birden fazla bakÄ±ÅŸ aÃ§Ä±sÄ± gerekli
     * Ä°ÅŸ HayatÄ±: GÃ¼ncel trendler + kanÄ±tlanmÄ±ÅŸ metodlar
     * Bilim: Peer-reviewed makaleler Ã¶ncelikli
     * Genel Bilgi: Orta hÄ±zda deÄŸiÅŸen

2. **Tarih ve Teknoloji Olgunluk Analizi (Ã‡OK Ã–NEMLÄ°):**
   - Ä°Ã§erikteki tarih bilgilerini tespit et
   - Teknoloji tÃ¼rÃ¼nÃ¼ belirle ve deÄŸiÅŸim hÄ±zÄ±nÄ± deÄŸerlendir:
     * HÄ±zlÄ± deÄŸiÅŸen teknolojiler (AI modelleri, mobil iÅŸlemciler, sosyal medya): 6 ay Ã¶ncesi = ESKÄ°
     * Orta hÄ±zda deÄŸiÅŸen teknolojiler (web frameworkleri, bulut servisleri): 2 yÄ±l Ã¶ncesi = ESKÄ°
     * YavaÅŸ deÄŸiÅŸen teknolojiler (veritabanlarÄ±, networking, matematik): 5 yÄ±l Ã¶ncesi = HALA GEÃ‡ERLÄ°
     * Ã‡ok yavaÅŸ deÄŸiÅŸen teknolojiler (programlama dilleri, iÅŸletim sistemi Ã§ekirdekleri): 10 yÄ±l Ã¶ncesi = HALA GEÃ‡ERLÄ°
     * Bilimsel araÅŸtÄ±rma: 5 yÄ±l Ã¶ncesi = HALA GEÃ‡ERLÄ°  
     * Genel bilgi: 2 yÄ±l Ã¶ncesi = ESKÄ°

3. **Kaynak Kalitesi ve TarafsÄ±zlÄ±k:**
   - Domain gÃ¼venilirliÄŸi
   - Ä°Ã§erik objektifliÄŸi vs subjektifliÄŸi
   - Spam/clickbait belirtileri
   - Siyasi/ideolojik Ã¶nyargÄ± kontrol et
   - Birden fazla bakÄ±ÅŸ aÃ§Ä±sÄ± sunuyor mu?
   - KanÄ±t ve referans kalitesi

4. **Konu-Spesifik Kriterler:**
   - Tarih/Psikoloji: Akademik kaynak mÄ±? Peer-reviewed mÄ±?
   - Siyaset: TarafsÄ±z mÄ±? FarklÄ± gÃ¶rÃ¼ÅŸleri de sunuyor mu?
   - Ä°ÅŸ HayatÄ±: Pratik deneyim var mÄ±? GerÃ§ek vaka Ã§alÄ±ÅŸmalarÄ± var mÄ±?
   - Bilim: Bilimsel yÃ¶ntem kullanÄ±lmÄ±ÅŸ mÄ±? Veriler doÄŸrulanabilir mi?

5. **Konu UygunluÄŸu:**
   - Ä°Ã§erik konuyla ne kadar alakalÄ±?
   - GÃ¼ncel bilgiler iÃ§eriyor mu?
   - Derinlemesine analiz var mÄ±?

SADECE BU FORMATTA CEVAP VER:
GÃ¼venilirlik: [0-100 arasÄ± skor]
Tarih: [iÃ§eriÄŸin ne zaman yazÄ±ldÄ±ÄŸÄ±nÄ± tahmin et]
Konu_TÃ¼rÃ¼: [teknoloji/tarih/psikoloji/siyaset/iÅŸ_hayatÄ±/bilim/genel]
TarafsÄ±zlÄ±k: [tarafsÄ±z/Ã¶nyargÄ±lÄ±/belirsiz]
Sebep: [tarih + konu tÃ¼rÃ¼ + tarafsÄ±zlÄ±k + kalite deÄŸerlendirmesi]
"""
            
            response = await self.call_local_model(
                reliability_prompt,
                "Sen kaynak gÃ¼venilirliÄŸi uzmanÄ±sÄ±n. Web sitelerinin gÃ¼venilirliÄŸini objektif olarak deÄŸerlendirirsin.",
                max_tokens=200
            )
            
            # Response'u parse et
            lines = response.strip().split('\n')
            reliability_score = 50  # default
            source_date = "Bilinmiyor"
            topic_type = "bilinmiyor"
            neutrality = "belirsiz"
            reason = "DeÄŸerlendirme yapÄ±lamadÄ±"
            
            for line in lines:
                if line.startswith('GÃ¼venilirlik:'):
                    try:
                        score_text = line.split(':')[1].strip()
                        reliability_score = int(''.join(filter(str.isdigit, score_text)))
                    except:
                        pass
                elif line.startswith('Tarih:'):
                    source_date = line.split(':', 1)[1].strip()
                elif line.startswith('Konu_TÃ¼rÃ¼:'):
                    topic_type = line.split(':', 1)[1].strip()
                elif line.startswith('TarafsÄ±zlÄ±k:'):
                    neutrality = line.split(':', 1)[1].strip()
                elif line.startswith('Sebep:'):
                    reason = line.split(':', 1)[1].strip()
            
            full_reason = f"Tarih: {source_date} | TÃ¼r: {topic_type} | TarafsÄ±zlÄ±k: {neutrality} | {reason}"
            return reliability_score, full_reason
            
        except Exception as e:
            logger.error(f"Source reliability evaluation failed: {e}")
            return 50, "DeÄŸerlendirme hatasÄ±"
    
    async def detect_conflicting_information(self, research_data, topic):
        """Ã‡eliÅŸkili bilgileri tespit eder"""
        try:
            if len(research_data) < 2:
                return "Yeterli kaynak yok"
            
            # Ä°lk 5 kaynaÄŸÄ± karÅŸÄ±laÅŸtÄ±r
            sources_to_compare = research_data[:5]
            
            comparison_prompt = f"""
'{topic}' konusu hakkÄ±nda aÅŸaÄŸÄ±daki farklÄ± kaynaklardan gelen bilgileri karÅŸÄ±laÅŸtÄ±r:

{chr(10).join([f"Kaynak {i+1}: {source['source']} (GÃ¼venilirlik: {source['reliability_score']}/100){chr(10)}{source['analysis']}{chr(10)}" for i, source in enumerate(sources_to_compare)])}

Bu kaynaklar arasÄ±nda Ã§eliÅŸkili bilgiler var mÄ±? Ã–zellikle dikkat et:
1. Ã‡eliÅŸkili bilgileri belirt
2. Hangi kaynak daha gÃ¼venilir gÃ¶rÃ¼nÃ¼yor?
3. Siyasi/ideolojik Ã¶nyargÄ± var mÄ±?
4. FarklÄ± bakÄ±ÅŸ aÃ§Ä±larÄ± dengeli sunuluyor mu?
5. DoÄŸru bilgi hangisi olabilir?

SADECE BU FORMATTA CEVAP VER:
Ã‡eliÅŸki: [Var/Yok]
Detay: [Ã§eliÅŸkili bilgiler varsa aÃ§Ä±klama]
TarafsÄ±zlÄ±k: [TÃ¼m kaynaklar tarafsÄ±z mÄ±?]
Ã–neri: [en gÃ¼venilir ve tarafsÄ±z bilgi hangisi]
"""
            
            response = await self.call_local_model(
                comparison_prompt,
                "Sen fact-checking uzmanÄ±sÄ±n. FarklÄ± kaynaklarÄ±n bilgilerini karÅŸÄ±laÅŸtÄ±rÄ±p Ã§eliÅŸkileri tespit edersin.",
                max_tokens=500
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Conflicting information detection failed: {e}")
            return "KarÅŸÄ±laÅŸtÄ±rma analizi yapÄ±lamadÄ±"
    
    async def extract_specific_data(self, content, topic):
        """Ä°Ã§erikten spesifik sayÄ±sal verileri Ã§Ä±karÄ±r"""
        try:
            extraction_prompt = f"""
'{topic}' konusu ile ilgili bu iÃ§erikten spesifik sayÄ±sal verileri Ã§Ä±kar:

Ä°Ã§erik: {content[:1000]}

ğŸ¯ Ã‡IKARILACAK VERÄ°LER:

1. **SAYISAL VERÄ°LER:**
   â€¢ TÃ¼m sayÄ±larÄ± ve birimlerini belirt (GB, TB, PB, kg, cm, $, %, yÄ±l, adet, vb.)
   â€¢ Ã–rnek: "11 petabayt", "100 sunucu", "2019 yÄ±lÄ±", "$10 milyon"

2. **HESAPLAMALAR:**
   â€¢ Matematiksel iÅŸlemler varsa gÃ¶ster
   â€¢ Ã–rnek: "4403 PB Ã· 11 PB = 400 kez izlenme"

3. **KARÅILAÅTIRMALAR:**
   â€¢ ArtÄ±ÅŸ/azalÄ±ÅŸ oranlarÄ±
   â€¢ Ã–rnek: "%25 artÄ±ÅŸ", "3 kat daha bÃ¼yÃ¼k"

SADECE BU FORMATTA CEVAP VER:
SayÄ±sal_Veriler: [tÃ¼m sayÄ±sal veriler listesi]
Hesaplamalar: [matematik iÅŸlemler varsa]
KarÅŸÄ±laÅŸtÄ±rmalar: [oranlar ve trendler]
"""
            
            response = await self.call_local_model(
                extraction_prompt,
                "Sen veri Ã§Ä±karma uzmanÄ±sÄ±n. Metinlerden sayÄ±sal bilgileri Ã§Ä±karÄ±rsÄ±n.",
                max_tokens=300
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Specific data extraction failed: {e}")
            return "Veri Ã§Ä±karma iÅŸlemi baÅŸarÄ±sÄ±z"
        
    async def call_local_model(self, prompt, system_prompt="", max_tokens=3000, thinking_process_prompt=""):
        """Lokal modeli asenkron olarak Ã§aÄŸÄ±rÄ±r - Ollama ve LM Studio desteÄŸi"""
        try:
            logger.info(f"call_local_model called with model_source: '{self.model_source}', model_name: '{self.model_name}'")
            import socket
            
            # Docker container'dan host'a eriÅŸim iÃ§in en gÃ¼venilir yÃ¶ntem
            host_ip = os.environ.get('OLLAMA_HOST_IP') or socket.gethostbyname('host.docker.internal')

            async with aiohttp.ClientSession() as session:
                logger.info(f"Model source comparison: '{self.model_source}' == 'Ollama' ? {self.model_source == 'Ollama'}")
                if self.model_source == "Ollama":
                    try:
                        
                        ollama_url = f"http://{host_ip}:11434/api/generate"
                        ollama_payload = {
                            "model": self.model_name,
                            "prompt": f"{thinking_process_prompt}\n\nUser: {prompt}\n\nAssistant:",
                            "system": system_prompt,
                            "stream": False,
                            "options": {
                                "temperature": 0.3,
                                "num_predict": max_tokens
                            }
                        }
                        
                        async with session.post(ollama_url, json=ollama_payload, timeout=300) as response:
                            logger.info(f"Ollama request to {ollama_url} with payload: {ollama_payload}")
                            logger.info(f"Ollama response status: {response.status}")
                            if response.status == 200:
                                data = await response.json()
                                return data.get('response', '').strip()
                            else:
                                error_text = await response.text()
                                logger.error(f"Ollama HTTP {response.status} error: {error_text}")
                                return f"Ollama hatasÄ±: {response.status} - {error_text}"
                    except Exception as ollama_error:
                        return f"Ollama hatasÄ±: {ollama_error}"
                
                elif self.model_source == "LM Studio":
                    try:
                        lm_studio_url = f"http://{host_ip}:1234/v1/chat/completions"
                        lm_payload = {
                            "model": self.model_name,
                            "messages": [
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": prompt}
                            ],
                            "temperature": 0.3,
                            "max_tokens": max_tokens,
                            "stream": False
                        }
                        
                        async with session.post(lm_studio_url, json=lm_payload, timeout=120) as response:
                            if response.status == 200:
                                data = await response.json()
                                content = data['choices'][0]['message']['content']
                                return content.strip()
                            else:
                                return f"LM Studio hatasÄ±: {response.status}"
                    except Exception as lm_error:
                        # LM Studio Ã§alÄ±ÅŸmÄ±yorsa Ollama'ya fallback yap
                        logger.warning(f"LM Studio eriÅŸilemez, Ollama'ya geÃ§iliyor: {lm_error}")
                        try:
                            ollama_url = f"http://{host_ip}:11434/api/generate"
                            ollama_payload = {
                                "model": self.model_name,
                                "prompt": f"{system_prompt}\n\nUser: {prompt}\n\nAssistant:",
                                "stream": False,
                                "options": {
                                    "temperature": 0.3,
                                    "num_predict": max_tokens
                                }
                            }
                            
                            async with session.post(ollama_url, json=ollama_payload, timeout=300) as response:
                                if response.status == 200:
                                    data = await response.json()
                                    return data.get('response', '').strip()
                                else:
                                    return f"Hem LM Studio hem Ollama eriÅŸilemez"
                        except:
                            return f"Model baÄŸlantÄ± hatasÄ±: LM Studio Ã§alÄ±ÅŸmÄ±yor, Ollama'ya da eriÅŸilemedi"
                
                else: # Kaynak bilinmiyorsa Ã¶nce LM Studio dene, sonra Ollama
                    try:
                        lm_studio_url = f"http://{host_ip}:1234/v1/chat/completions"
                        lm_payload = {
                            "model": self.model_name,
                            "messages": [
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": prompt}
                            ],
                            "temperature": 0.3,
                            "max_tokens": max_tokens,
                            "stream": False
                        }
                        
                        async with session.post(lm_studio_url, json=lm_payload, timeout=120) as response:
                            if response.status == 200:
                                data = await response.json()
                                content = data['choices'][0]['message']['content']
                                return content.strip()
                    except:
                        pass
                    
                    try:
                        ollama_url = f"http://{host_ip}:11434/api/generate"
                        ollama_payload = {
                            "model": self.model_name,
                            "prompt": f"{thinking_process_prompt}\n\nUser: {prompt}\n\nAssistant:",
                            "system": system_prompt,
                            "stream": False,
                            "options": {
                                "temperature": 0.3,
                                "num_predict": max_tokens
                            }
                        }
                        
                        async with session.post(ollama_url, json=ollama_payload, timeout=300) as response:
                            if response.status == 200:
                                data = await response.json()
                                return data.get('response', '').strip()
                    except:
                        pass
                
                return "Model baÄŸlantÄ± hatasÄ±: Hem LM Studio hem Ollama eriÅŸilemez"

        except Exception as e:
            return f"Model baÄŸlantÄ± hatasÄ±: {str(e)}"

    async def search_web(self, query, max_results=12):
        """Web aramasÄ± yapar - Google Ã¶ncelikli, Tavily fallback"""
        try:
            # Google search ile baÅŸla
            try:
                import asyncio
                import concurrent.futures
                from googlesearch import search
                
                def sync_search():
                    return list(search(query, num_results=max_results, lang=self.detect_language(query)))
                
                # Run sync search in thread pool
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    search_results = await asyncio.get_event_loop().run_in_executor(
                        executor, sync_search
                    )
                
                results = []
                for i, url in enumerate(search_results):
                    if i >= max_results:
                        break
                        
                    results.append({
                        'title': url, # Google search does not provide titles directly
                        'body': '', # Google search does not provide snippets directly
                        'url': url,
                        'source': 'Google'
                    })
                
                if results:
                    return results
                    
            except Exception as google_error:
                logger.warning(f"Google search error, falling back to Tavily: {google_error}")
            
            # Tavily search fallback
            try:
                from src.libs.utils.tavily_search import atavily_search_results
                
                search_data = await atavily_search_results(
                    query=query, 
                    max_results=max_results, 
                    include_raw=True
                )
                
                results = []
                for i, result in enumerate(search_data.results):
                    if i >= max_results:
                        break
                        
                    results.append({
                        'title': result.title,
                        'body': result.content,
                        'url': result.link,
                        'source': 'Tavily (Fallback)'
                    })
                
                return results
                    
            except Exception as tavily_error:
                logger.error(f"Both Google and Tavily search failed: {tavily_error}")
            
            return []
            
        except Exception as e:
            logger.error(f"Web search error: {e}")
            return []

    async def extract_content_from_url(self, url, title):
        """URL'den iÃ§erik Ã§eker"""
        try:
            await self.websocket.send_json({
                "type": "progress", 
                "step": 0.3, 
                "message": f"ğŸ“– {title[:50]}... sayfasÄ± okunuyor"
            })
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        
                        # Basit HTML temizleme
                        try:
                            from bs4 import BeautifulSoup
                            soup = BeautifulSoup(html, 'html.parser')
                        except ImportError:
                            # BeautifulSoup yoksa basit text extract
                            import re
                            # HTML taglerini kaldÄ±r
                            text = re.sub('<[^<]+?>', '', html)
                            return text[:3000]
                        
                        # Script ve style etiketlerini kaldÄ±r
                        for script in soup(["script", "style"]):
                            script.decompose()
                        
                        # Text'i al
                        text = soup.get_text()
                        
                        # SatÄ±rlarÄ± temizle
                        lines = (line.strip() for line in text.splitlines())
                        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                        text = ' '.join(chunk for chunk in chunks if chunk)
                        
                        # Ä°lk 3000 karakteri al
                        return text[:3000]
            
        except Exception as e:
            logger.error(f"Content extraction error for {url}: {e}")
            return ""
        
        return ""

    async def check_relevance(self, topic, search_result):
        """Arama sonucunun konuyla ilgili olup olmadÄ±ÄŸÄ±nÄ± kontrol eder"""
        try:
            title = search_result.get('title', '')
            snippet = search_result.get('snippet', '')
            
            relevance_prompt = f"""
ARAÅTIRMA KONUSU: {topic}

ARAMA SONUCU:
BaÅŸlÄ±k: {title}
Ä°Ã§erik: {snippet}

Bu arama sonucu, araÅŸtÄ±rma konusuyla ilgili mi?

SADECE "EVET" veya "HAYIR" cevabÄ± ver.
EÄŸer iÃ§erik konuyla alakalÄ± ise EVET, alakasÄ±z ise HAYIR.
"""
            
            relevance = await self.call_local_model(relevance_prompt, "Sen iÃ§erik analiz uzmanÄ±sÄ±n.")
            return "EVET" in relevance.upper()
            
        except Exception as e:
            logger.error(f"Relevance check error: {e}")
            return True  # Hata durumunda kabul et

    def detect_language(self, text):
        """Metindeki dili algÄ±lar"""
        # TÃ¼rkÃ§e karakterler
        turkish_chars = ['Ã§', 'ÄŸ', 'Ä±', 'ÅŸ', 'Ã¼', 'Ã¶', 'Ã‡', 'Ä', 'Ä°', 'Å', 'Ãœ', 'Ã–']
        turkish_words = ['ve', 'ile', 'bir', 'bu', 'ÅŸu', 'o', 'nedir', 'nasÄ±l', 'ne', 'hangi', 'en', 'iyi', 'gÃ¼ncel']
        
        # FransÄ±zca karakterler ve kelimeler
        french_chars = ['Ã©', 'Ã¨', 'Ãª', 'Ã«', 'Ã ', 'Ã¢', 'Ã¤', 'Ã§', 'Ã¹', 'Ã»', 'Ã¼', 'Ã´', 'Ã¶', 'Ã®', 'Ã¯', 'Ã¿']
        french_words = ['le', 'la', 'les', 'un', 'une', 'de', 'du', 'des', 'et', 'ou', 'est', 'ce', 'que', 'qui', 'comment', 'quel', 'quelle']
        
        # Almanca karakterler ve kelimeler
        german_chars = ['Ã¤', 'Ã¶', 'Ã¼', 'ÃŸ', 'Ã„', 'Ã–', 'Ãœ']
        german_words = ['der', 'die', 'das', 'und', 'oder', 'ist', 'was', 'wie', 'welche', 'beste', 'aktuelle']
        
        text_lower = text.lower()
        
        # Karakter kontrolÃ¼
        if any(char in text for char in turkish_chars):
            return 'tr'
        elif any(char in text for char in french_chars):
            return 'fr'
        elif any(char in text for char in german_chars):
            return 'de'
        
        # Kelime kontrolÃ¼
        turkish_score = sum(1 for word in turkish_words if word in text_lower)
        french_score = sum(1 for word in french_words if word in text_lower)
        german_score = sum(1 for word in german_words if word in text_lower)
        
        if turkish_score > 0:
            return 'tr'
        elif french_score > 0:
            return 'fr'
        elif german_score > 0:
            return 'de'
        
        return 'en'  # VarsayÄ±lan Ä°ngilizce

    async def research_topic(self, topic):
        """GerÃ§ek deep research yapar"""
        
        await self.websocket.send_json({
            "type": "progress", 
            "step": 0.05, 
            "message": f"ğŸš€ '{topic}' konusu iÃ§in deep research baÅŸlÄ±yor..."
        })
        
        # 1. Dil algÄ±lama
        detected_lang = self.detect_language(topic)
        lang_names = {'tr': 'TÃ¼rkÃ§e', 'en': 'Ä°ngilizce', 'fr': 'FransÄ±zca', 'de': 'Almanca'}
        
        await self.websocket.send_json({
            "type": "progress", 
            "step": 0.08, 
            "message": f"ğŸŒ {lang_names.get(detected_lang, 'Ä°ngilizce')} dili algÄ±landÄ±"
        })
        
        # 2. Konu analizi ve arama sorgularÄ± oluÅŸtur
        current_date = datetime.now().strftime("%d %B %Y")
        
        # Ã–nce algÄ±lanan dilde arama sorgularÄ±
        primary_prompt = f"""
ARAÅTIRMA KONUSU: {topic}

BUGÃœNÃœN TARÄ°HÄ°: {current_date}

Bu tarihe gÃ¶re kendi karar ver: hangi zaman dilimindeki bilgileri aramalÄ±sÄ±n?
- EÄŸer 2025 yÄ±lÄ±ndaysak, 2024-2025 arasÄ± bilgiler gÃ¼ncel
- EÄŸer gÃ¼ncel teknoloji/AI/fiyat/trend araÅŸtÄ±rmasÄ± ise, son 6-12 ay Ã¶nemli
- EÄŸer tarihi konu ise, yÄ±l Ã¶nemli deÄŸil

Bu analizi yaptÄ±ktan sonra {lang_names.get(detected_lang, 'Ä°ngilizce')} dilinde 3 farklÄ± arama terimi ile araÅŸtÄ±r.

KONU ANALÄ°ZÄ° VE ARAMA STRATEJÄ°SÄ°:
1. Ana konuyu doÄŸru anla ve spesifik terimleri kullan
2. Tarihe gÃ¶re uygun zaman aralÄ±ÄŸÄ±nÄ± belirle
3. Teknik detaylar iÃ§in spesifik terimler kullan (Ã¶rn: "petabytes", "servers", "infrastructure")
4. Ä°statistik ve rakam odaklÄ± arama yap ("statistics", "data", "numbers")
5. GÃ¼venilir kaynaklarÄ± hedefle ("reddit", "technical interview", "official report")
6. KarÅŸÄ±laÅŸtÄ±rma ve deÄŸerlendirme terimleri kullan ("vs", "comparison", "analysis")
7. Her sorgu farklÄ± aÃ§Ä±dan yaklaÅŸsÄ±n
8. Ã–ZEL: AI/teknoloji konularÄ±nda "January 2025", "2025 release", "latest models" gibi Ã§ok gÃ¼ncel terimler kullan

SADECE ARAMA TERÄ°MLERÄ°NÄ° VER:
1.
2.
3.
"""
        
        # Sonra Ä°ngilizce arama sorgularÄ± (ana dil Ä°ngilizce deÄŸilse)
        secondary_prompt = f"""
ARAÅTIRMA KONUSU: {topic}

BUGÃœNÃœN TARÄ°HÄ°: {current_date}

Bu tarihe gÃ¶re kendi karar ver ve bu konuyu Ä°ngilizce olarak 2 farklÄ± arama terimi ile araÅŸtÄ±r.

KONU ANALÄ°ZÄ° VE ARAMA STRATEJÄ°SÄ°:
1. Ana konuyu Ä°ngilizce karÅŸÄ±lÄ±ÄŸÄ± ile anla
2. Tarihe gÃ¶re uygun terimleri kullan ("latest", "current", "2024", "2025" vb.)
3. Konunun Ã¶zelliÄŸine gÃ¶re zaman aralÄ±ÄŸÄ±nÄ± belirle

SADECE ARAMA TERÄ°MLERÄ°NÄ° VER:
1.
2.
"""
        
        await self.websocket.send_json({
            "type": "progress", 
            "step": 0.1, 
            "message": "ğŸ§  AraÅŸtÄ±rma stratejisi belirleniyor..."
        })
        
        # Ã–nce ana dilde arama sorgularÄ± al
        try:
            logger.info(f"Generating primary language queries...")
            primary_queries_text = await self.call_local_model(
                primary_prompt, 
                "Sen araÅŸtÄ±rma uzmanÄ±sÄ±n. Verilen konular iÃ§in etkili arama sorgularÄ± oluÅŸturursun."
            )
            logger.info(f"Primary queries generated successfully")
        except Exception as e:
            logger.error(f"Primary query generation failed: {e}")
            primary_queries_text = "1. ana konu\n2. genel araÅŸtÄ±rma"
        
        # Ana dil Ä°ngilizce deÄŸilse, Ä°ngilizce sorgular da al
        secondary_queries_text = ""
        if detected_lang != 'en':
            try:
                logger.info(f"Generating secondary English queries...")
                secondary_queries_text = await self.call_local_model(
                    secondary_prompt, 
                    "Sen araÅŸtÄ±rma uzmanÄ±sÄ±n. Verilen konular iÃ§in etkili arama sorgularÄ± oluÅŸturursun."
                )
                logger.info(f"Secondary queries generated successfully")
            except Exception as e:
                logger.error(f"Secondary query generation failed: {e}")
                secondary_queries_text = ""
        
        # SorgularÄ± parse et - duplikasyonlarÄ± Ã¶nle
        search_queries = []
        seen_queries = set()
        
        # Ã–nce ana dil sorgularÄ±
        for line in primary_queries_text.split('\n'):
            line = line.strip()
            if line and (line[0].isdigit() or line.startswith('-')):
                query = line.split('.', 1)[-1].strip()
                if query and query.lower() not in seen_queries and len(query) > 5:
                    search_queries.append(query)
                    seen_queries.add(query.lower())
        
        # Sonra Ä°ngilizce sorgularÄ± (varsa)
        if secondary_queries_text:
            for line in secondary_queries_text.split('\n'):
                line = line.strip()
                if line and (line[0].isdigit() or line.startswith('-')):
                    query = line.split('.', 1)[-1].strip()
                    if query and query.lower() not in seen_queries and len(query) > 5:
                        search_queries.append(query)
                        seen_queries.add(query.lower())
        
        # Fallback sorgularÄ± ekle
        if not search_queries:
            search_queries = [topic, f"{topic} 2025", f"latest {topic}"]
        
        # Ä°lk 5 sorguyu kullan
        search_queries = search_queries[:5]
        
        # 2. Web aramasÄ± yap
        all_search_results = []
        
        for i, query in enumerate(search_queries):
            results = await self.search_web(query, max_results=8)
            
            # SonuÃ§larÄ± filtrele - sadece ilgili olanlarÄ± al
            filtered_results = []
            for result in results:
                if await self.check_relevance(topic, result):
                    filtered_results.append(result)
            
            all_search_results.extend(filtered_results)
            await asyncio.sleep(1)  # Rate limiting
        
        # 3. Ä°Ã§erikleri analiz et
        research_data = []
        
        for i, result in enumerate(all_search_results[:20]):  # Ä°lk 20 sonuÃ§
            # KullanÄ±cÄ±ya hangi siteyi incelediÄŸini gÃ¶ster
            await self.websocket.send_json({
                "type": "message", 
                "message": f"{i+1}. {result['url']} - Ä°nceleniyor..."
            })
            
            # Ä°Ã§erik Ã§ek
            if result['url']:
                content = await self.extract_content_from_url(result['url'], result['title'])
                result['content'] = content
                
                # Kaynak gÃ¼venilirliÄŸini deÄŸerlendir
                reliability_score, reason = await self.evaluate_source_reliability(
                    result['url'], result['title'], content, topic
                )
                result['reliability_score'] = reliability_score
                result['reliability_reason'] = reason
            
            # Model ile analiz et
            if result.get('content') or result.get('body'):
                analysis_text = result.get('content', result.get('body', ''))
                
                analysis_prompt = f"""
Bu web kaynaÄŸÄ±ndaki bilgileri analiz et ve '{topic}' konusu ile ilgili Ã¶nemli bilgileri Ã¶zetle:

Kaynak: {result['title']}
URL: {result['url']}

Ä°Ã§erik:
{analysis_text[:2000]}

ğŸ” SPESIFIK VERÄ° Ã‡IKARMA TALÄ°MATLARI:

1. **SAYISAL VERÄ°LER (MUTLAKA BELIRT):**
   â€¢ Teknik: GB, TB, PB, MB, KB, CPU, RAM, sunucu sayÄ±sÄ±, bant geniÅŸliÄŸi
   â€¢ Fiziksel: kg, g, cm, m, km, litre, ml, derece, watt, volt
   â€¢ Finansal: dolar, euro, lira, milyar, milyon, maliyet, bÃ¼tÃ§e
   â€¢ Zaman: yÄ±l, ay, gÃ¼n, saat, dakika, saniye
   â€¢ Performans: fps, bit rate, hÄ±z, frekans, oran, yÃ¼zde

2. **HESAPLAMALAR ve KARÅILAÅTIRMALAR:**
   â€¢ Matematik iÅŸlemler yap (Ã¶rn: 4403 PB Ã· 11 PB = 400 kez)
   â€¢ OranlarÄ± belirt (Ã¶rn: %25 artÄ±ÅŸ, 3 kat daha bÃ¼yÃ¼k)
   â€¢ Trend analizi (artÄ±ÅŸ/azalÄ±ÅŸ, zaman iÃ§indeki deÄŸiÅŸim)

3. **KAYNAK ve TARÄ°H BÄ°LGÄ°SÄ°:**
   â€¢ Bu veri ne zaman yayÄ±nlandÄ±?
   â€¢ GÃ¼ncel mi yoksa eski mi?
   â€¢ Resmi kaynak mÄ± yoksa tahmin mi?

4. **ELEÅTÄ°REL ANALÄ°Z:**
   â€¢ Hangi varsayÄ±mlarla sonuca ulaÅŸÄ±ldÄ±?
   â€¢ SÄ±nÄ±rlamalar ve belirsizlikler neler?
   â€¢ FarklÄ± kaynaklarla tutarlÄ± mÄ±?

Ã–RNEK FORMAT:
"Kaynak X'e gÃ¶re, 2019'da 11 PB depolama kapasitesi vardÄ±. 2024 iÃ§in, veri transferindeki artÄ±ÅŸa dayanarak (4403 PB'dan 5500 PB'ye), yaklaÅŸÄ±k 13.75 PB tahmin ediliyor. Ancak bu hesaplama izlenme oranÄ±nÄ±n sabit kaldÄ±ÄŸÄ±nÄ± varsayar."

Sadece konuyla ilgili bilgileri Ã¶zetle, kaynak adÄ±nÄ± da belirt.
"""
                
                analysis = await self.call_local_model(
                    analysis_prompt,
                    "Sen araÅŸtÄ±rma analistisin. Web kaynaklarÄ±ndaki bilgileri Ã¶zetlersin.",
                    max_tokens=500
                )
                
                # Spesifik veri Ã§Ä±karma
                specific_data = await self.extract_specific_data(analysis_text, topic)
                
                if analysis and "hatasÄ±" not in analysis.lower():
                    research_data.append({
                        'source': result['title'],
                        'url': result['url'],
                        'analysis': analysis,
                        'specific_data': specific_data,
                        'reliability_score': result.get('reliability_score', 50),
                        'reliability_reason': result.get('reliability_reason', 'Bilinmiyor')
                    })
                    
                    # KullanÄ±cÄ±ya sonuÃ§ bulduÄŸunu gÃ¶ster
                    await self.websocket.send_json({
                        "type": "message", 
                        "message": f"   âœ… FaydalÄ± bilgi bulundu"
                    })
                else:
                    # SonuÃ§ bulunamadÄ± mesajÄ±
                    await self.websocket.send_json({
                        "type": "message", 
                        "message": f"   âŒ KullanÄ±labilir bilgi bulunamadÄ±"
                    })
        
        # 4. KaynaklarÄ± gÃ¼venilirlik skoruna gÃ¶re sÄ±rala
        research_data.sort(key=lambda x: x['reliability_score'], reverse=True)
        
        # DÃ¼ÅŸÃ¼k gÃ¼venilirlik skorlu kaynaklarÄ± filtrele (30'un altÄ±)
        filtered_research_data = [item for item in research_data if item['reliability_score'] >= 30]
        
        await self.websocket.send_json({
            "type": "message", 
            "message": f"\nğŸ“‹ Toplam {len(filtered_research_data)} gÃ¼venilir kaynak bulundu"
        })
        
        await self.websocket.send_json({
            "type": "message", 
            "message": "\nğŸ¤– Rapor hazÄ±rlanÄ±yor...\n"
        })
        
        # Ã‡eliÅŸkili bilgileri tespit et
        conflicting_info = await self.detect_conflicting_information(filtered_research_data, topic)
        
        # TÃ¼m analiz sonuÃ§larÄ±nÄ± birleÅŸtir - gÃ¼venilirlik skoru ile
        combined_research = "\n\n".join([
            f"**Kaynak: {item['source']}** (GÃ¼venilirlik: {item['reliability_score']}/100)\nURL: {item['url']}\nGÃ¼venilirlik Notu: {item['reliability_reason']}\n{item['analysis']}"
            for item in filtered_research_data
        ])
        
        # Kaynak listesini de ekle
        source_list = "\n".join([
            f"- {item['source']}: {item['url']} (GÃ¼venilirlik: {item['reliability_score']}/100)"
            for item in filtered_research_data
        ])
        
        # EÄŸer hiÃ§ araÅŸtÄ±rma verisi yoksa, model bilgisiyle fallback yap
        if not research_data:
            fallback_prompt = f"""
'{topic}' konusu hakkÄ±nda mevcut bilgilerini kullanarak kapsamlÄ± bir analiz yap:

- Temel tanÄ±m ve kavramlar
- Ã–nemli Ã¶zellikler ve karakteristikler
- GÃ¼ncel durum ve geliÅŸmeler
- Gelecek beklentileri
- Pratik uygulamalar

DetaylÄ± ve bilgilendirici bir rapor hazÄ±rla.
"""
            fallback_research = await self.call_local_model(
                fallback_prompt,
                "Sen uzman araÅŸtÄ±rmacÄ±sÄ±sÄ±n. Konular hakkÄ±nda kapsamlÄ± analizler yaparsÄ±n.",
                max_tokens=2000
            )
            
            research_data.append({
                'source': 'AI Model Bilgi TabanÄ±',
                'url': 'N/A',
                'analysis': fallback_research
            })
            
            combined_research = f"**Kaynak: AI Model Bilgi TabanÄ±**\nURL: N/A\n{fallback_research}"
        
        final_prompt = f"""
AÅŸaÄŸÄ±daki araÅŸtÄ±rma sonuÃ§larÄ±nÄ± kullanarak '{topic}' konusu hakkÄ±nda TÃ¼rkÃ§e kapsamlÄ± bir rapor hazÄ±rla:

ARAÅTIRMA VERÄ°LERÄ°:
{combined_research}

Ã‡ELIÅKI ANALÄ°ZÄ°:
{conflicting_info}

KULLANILAN KAYNAKLAR:
{source_list}

DÃœÅÃœNME ve ANALÄ°Z YAKLAÅIMI:
- Verileri eleÅŸtirel olarak sorgula
- Matematik hesaplamalar yap (varsa)
- VarsayÄ±mlarÄ± ve sÄ±nÄ±rlamalarÄ± belirt
- FarklÄ± kaynaklarÄ± karÅŸÄ±laÅŸtÄ±r
- ÅÃ¼pheyle yaklaÅŸ, "kesin deÄŸil" diyerek belirt
- Ä°nsan gibi dÃ¼ÅŸÃ¼n ve analiz et

RAPOR Ä°HTÄ°YAÃ‡LARI:
- KullanÄ±cÄ±nÄ±n sorusuna doÄŸrudan ve net cevap ver
- Sorulan spesifik deÄŸerleri (boyut, fiyat, tarih, teknik Ã¶zellikler vb.) mutlaka belirt
- FarklÄ± kaynaklardaki Ã§eliÅŸkili bilgileri karÅŸÄ±laÅŸtÄ±r ve en doÄŸru olanÄ±nÄ± seÃ§
- Kaynak gÃ¼venilirlik skorlarÄ±nÄ± dikkate al (yÃ¼ksek skorlu kaynaklarÄ± Ã¶ncelendir)
- Belirsiz veya eksik bilgiler varsa aÃ§Ä±kÃ§a belirt
- CevabÄ±nÄ± kanÄ±tlarla ve araÅŸtÄ±rma verileriyle destekle
- Tamamen TÃ¼rkÃ§e yazÄ±lmÄ±ÅŸ olmasÄ± (hiÃ§ Ä°ngilizce kelime kullanma)
- Net giriÅŸ bÃ¶lÃ¼mÃ¼
- Ana bulgular ve Ã¶nemli geliÅŸmeler  
- DetaylÄ± analiz ve deÄŸerlendirmeler
- SonuÃ§ ve gelecek Ã¶ngÃ¶rÃ¼leri
- Markdown formatÄ±nda dÃ¼zenli yapÄ±
- MUTLAKA rapor sonunda "Kaynaklar" bÃ¶lÃ¼mÃ¼ ekle ve tÃ¼m kaynak URL'lerini listele

DÄ°KKAT: Bilgi doÄŸruluÄŸu kritik Ã¶neme sahiptir. EÄŸer bir bilgi kesin deÄŸilse "tahmin ediyorum" veya "olasÄ±" gibi ifadeler kullan.
GÃœVEN: YÃ¼ksek gÃ¼venilirlik skorlu kaynaklarÄ± Ã¶ncelendir, dÃ¼ÅŸÃ¼k skorlu kaynaklara ÅŸÃ¼pheyle yaklaÅŸ.
ÅEFFAFLIK: TÃ¼m bilgilerin hangi kaynaktan geldiÄŸini belirt, URL'leri kullanÄ±cÄ±ya gÃ¶ster.

Ã–zellikle gÃ¼ncel geliÅŸmelere odaklanarak kapsamlÄ± ve bilimsel bir rapor oluÅŸtur. TÃ¼m metni TÃ¼rkÃ§e yaz.
"""

        thinking_process_prompt = "KullanÄ±cÄ±ya sadece bitmiÅŸ ve temizlenmiÅŸ raporu gÃ¶ster. Ã–n hazÄ±rlÄ±k veya dÃ¼ÅŸÃ¼nme sÃ¼recini rapora dahil etme."
        
        final_report = await self.call_local_model(
            final_prompt,
            "Sen uzman araÅŸtÄ±rmacÄ±sÄ±sÄ±n. Web araÅŸtÄ±rmasÄ± sonuÃ§larÄ±ndan kapsamlÄ±, profesyonel raporlar yazarsÄ±n.",
            max_tokens=4000,
            system_prompt=thinking_process_prompt
        )
        
        # 5. KullanÄ±lan kaynaklarÄ± txt dosyasÄ±na kaydet
        try:
            import os
            desktop_path = "/app/desktop"  # Docker'da host masaÃ¼stÃ¼ mount edildi
            research_path = "/app/research_results"  # Container iÃ§i results
            
            os.makedirs(desktop_path, exist_ok=True)
            os.makedirs(research_path, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_topic = "".join(c for c in topic if c.isalnum() or c in (' ', '-', '_')).rstrip()[:50]
            sources_filename = f"{timestamp}_{safe_topic.replace(' ', '_')}_sources.txt"
            
            # KaynaklarÄ± txt dosyasÄ±na kaydet
            desktop_sources_path = os.path.join(desktop_path, sources_filename)
            research_sources_path = os.path.join(research_path, sources_filename)
            
            sources_content = f"AraÅŸtÄ±rma Konusu: {topic}\n"
            sources_content += f"Tarih: {datetime.now().strftime('%d %B %Y, %H:%M')}\n"
            sources_content += f"Toplam Kaynak: {len(research_data)}\n"
            sources_content += f"Arama SorgularÄ±: {search_queries}\n\n"
            sources_content += "KULLANILAN KAYNAKLAR:\n"
            sources_content += "=" * 50 + "\n\n"
            
            for i, item in enumerate(research_data, 1):
                sources_content += f"{i}. {item['source']}\n"
                sources_content += f"   URL: {item['url']}\n"
                sources_content += f"   GÃ¼venilirlik: {item.get('reliability_score', 'N/A')}/100\n"
                sources_content += f"   Kaynak TÃ¼rÃ¼: {item.get('source', 'Bilinmiyor').split('(')[0].strip()}\n\n"
            
            # Hem masaÃ¼stÃ¼ne hem research_results'a kaydet
            try:
                with open(desktop_sources_path, 'w', encoding='utf-8') as f:
                    f.write(sources_content)
            except Exception as e:
                logger.error(f"Desktop sources save error: {e}")
            
            try:
                with open(research_sources_path, 'w', encoding='utf-8') as f:
                    f.write(sources_content)
            except Exception as e:
                logger.error(f"Research sources save error: {e}")
            
        except Exception as e:
            logger.error(f"Sources file save error: {e}")
            
        # 6. Dosya kaydet (rapor)
        try:
            filename = f"{timestamp}_{safe_topic.replace(' ', '_')}_deep_research.md"
            
            # Hem masaÃ¼stÃ¼ne hem research_results'a kaydet
            desktop_filepath = os.path.join(desktop_path, filename)
            research_filepath = os.path.join(research_path, filename)
            
            report_header = f"""# Derin AraÅŸtÄ±rma Raporu: {topic}

**AraÅŸtÄ±rma Tarihi:** {datetime.now().strftime('%d %B %Y, %H:%M')}
**AraÅŸtÄ±rma TÃ¼rÃ¼:** Web TabanlÄ± Derin AraÅŸtÄ±rma
**KullanÄ±lan Model:** {self.model_name}
**Toplam Kaynak:** {len(research_data)} web kaynaÄŸÄ±
**Arama SorgularÄ±:** {len(search_queries)} farklÄ± sorgu

---

"""
            
            source_list = "\n".join([
                f"- [{item['source']}]({item['url']})"
                for item in research_data
            ])
            
            full_report = report_header + final_report + f"\n\n## Kaynaklar\n\n{source_list}"
            
            # MasaÃ¼stÃ¼ne kaydet
            try:
                with open(desktop_filepath, 'w', encoding='utf-8') as f:
                    f.write(full_report)
            except Exception as e:
                logger.error(f"Desktop save error: {e}")
            
            # Research results'a da kaydet
            try:
                with open(research_filepath, 'w', encoding='utf-8') as f:
                    f.write(full_report)
            except Exception as e:
                logger.error(f"Research results save error: {e}")
            
        except Exception as e:
            final_report += f"\n\n**Not:** Dosya kaydetme hatasÄ±: {str(e)}"
        
        await self.websocket.send_json({
            "type": "message", 
            "message": "ğŸ‰ AraÅŸtÄ±rma tamamlandÄ±! Kaynaklar txt dosyasÄ±na kaydedildi."
        })
        
        return final_report